<!DOCTYPE html>
<html>
	<head>
		<title>VISION-FLAN</title>
        <link rel="stylesheet" href="./index.css">
        <script src="./index.js"></script>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="shortcut icon" href="./imgs/vision_flan_logo.jpg"/>
	</head>
	<body>
        <!-- NAVBAR -->
        <nav class="navbar">
            <div class="navbarmenu">
                <a class="visionFlan">VISION-FLAN</a>
            </div>
        </nav>
        
        <div class="major_section">
            <h1>Vision-Flan: Scaling Visual Instruction Tuning</h1>
            <p>Created by Virginia Tech's NLP Lab. Sep 13, 2023</p>
            <hr>
            <br>
            <center>
                <img src="./imgs/vision_flan_logo.jpg" width="400px">
                <i><p>Generated by <a href="https://ideogram.ai/">https://ideogram.ai/</a></p></i>
            </center>
            <br>
            <center>
                <div>
                    <a href="./tasks.html"><button><i class="fa fa-book"></i> Tasks</button></a>
                    <a href="#data_samples"><button><i class="fa fa-github"></i> Samples</button></a>
                    <a href="#download"><button><i class="fa fa-database"></i> Download</button></a>
                    <a href="#models"><button><i class="fa fa-arrow-circle-o-right"></i> Models</button></a>
                    <a href="#acknowledgement"><button><i class="fa fa-arrow-circle-o-right"></i> Acknowledgement</button></a>
                </div>
            </center>
            <br>
            <p class="dataset_description"> We introduce Vision-Flan, the largest human-annotated 
                visual instruction tuning dataset that consists of 210 diverse vision-language 
                tasks derived from xxx open-source datasets. Each task is equipped with an expert 
                written instruction and carefully designed templates for the input and output. The dataset encompasses 
                a wide range of tasks such as image captioning, visual question-answering, and 
                visual understanding. Vision-Flan is built to support various AI researches and applications that require the fusion of visual and 
                textual information, pushing the boundaries of understanding and interaction 
                between these two modality. Researchers and practitioners can 
                leverage this dataset to advance the state of the art vision-language models
                and develop innovative solutions in a wide range of domains.
            </p>
        </div><br><br><br><br>

        <div class="section_title" id="data_samples">
            <h1>Data Samples<hr></h1>
        </div>
        <div class="content-block" id="content-block">
            <p class="data_loading">DATA LOADING . . .</p>
        </div>

        <div class="section_title">
            <p class="dataset_description">Each data sample consists of 3 primary elements: Image, Instruction, and Ouput.</p>
            <ul>
                <li><p><b>Image:</b> An image that is used as reference when solving a given instruction.</p></li>
                <li><p><b>Instruction:</b> A command representing a task that must be completed by the model/agent.</p></li>
                <li><p><b>Output:</b> The answer to the instruction given the provided image.</p></li>
            </ul>
            <br><br><br>
        </div>


        <div class="section_title" id="collection_annotation">
            <h1>Data Collection and Annotation<hr></h1>
            <br><br>
            <center><img src="./imgs/pipeline_snip.png" width="700px"></center>
            <br><br>
            <p class="dataset_description">In the data gathering phase of this research endeavor, a systematic 
                and meticulous seven-step process was followed. It commenced with an exhaustive search for 
                pertinent datasets, leveraging advanced search algorithms to pinpoint the most relevant 
                sources. Subsequently, these datasets were downloaded and subjected to rigorous preprocessing 
                procedures, including data cleaning and normalization, to ensure their suitability for analysis. 
                A critical juncture was reached when potential tasks were brainstormed, offering clarity and 
                direction to the data collection process. An iterative cycle of instruction refinement and 
                feedback was perpetuated, where instructions and templates were revised to eliminate ambiguities. 
                The resulting code and preprocessed data were securely uploaded to the cloud for easy access and 
                collaboration. Rigorous quality control measures included thorough checks on input and output 
                correctness, coupled with periodic rewrites of instructions and templates to address evolving 
                challenges. Additionally, advanced image analysis techniques were employed to sift out 
                low-quality images, and annotations underwent meticulous scrutiny to eliminate any 
                inaccuracies or inconsistencies. This comprehensive data gathering process ensured 
                the acquisition of high-quality, reliable, and precise datasets for the research at hand.
            </p>
        </div>
        
        <br><br><br><br>
        <div class="section_title" id="download">
            <h1>Download<hr></h1>
            <table>
                <tr>
                    <th>File</th>
                    <th>Size on Disk</th>
                    <th>Sample Size</th>
                </tr>
                <tr>
                    <td><a href="">annotations_1k.json</a></td>
                    <td>100GB</td>
                    <td>35k</td>
                </tr>
                <tr>
                    <td><a href="">images_1k.zip</a></td>
                    <td>50GB</td>
                    <td>12k</td>
                </tr>
                <tr>
                    <td><a href="">instructions.json</a></td>
                    <td>1TB</td>
                    <td>300k</td>
                </tr>
            </table><br><br>
            <p class="dataset_description">This text section will describe the different files we have available for download in the table above.
                We have a set of annotations, images, and instructions available for download. Each file is related to one another, so we recommend
                using each file together for optimal performance.
            </p>
        </div>
        
        

        <br><br><br><br>
        <div class="section_title" id="models">
            <h1>Models<hr></h1>
            <table>
                <tr>
                    <th>File</th>
                    <th>Langugae model</th>
                    <th>Turning Strategy</th>
                </tr>
                <tr>
                    <td><a href="">vision-flan_llava</a></td>
                    <td>Vicuna_v1.3_7B</td>
                    <td>Lora</td>
                </tr>
                <tr>
                    <td><a href="">vision-flan_blip2_xl</a></td>
                    <td>Flan_T5_xl</td>
                    <td>Qformer</td>
                </tr>
                <tr>
                    <td><a href="">vision-flan_blip2_xxl</a></td>
                    <td>Flan_T5_xxl</td>
                    <td>Qformer</td>
                </tr>
            </table><br><br>
            <p class="dataset_description">This text section will describe the different models we have available for download from huggingface in the table above.
                We have a vision-flan using vicuna and two flan-t5 models. These models can be used independantly of one another. If you would like to see model performance
                and evaluation, please see the section below.
            </p>
        </div>
        
        <div class="section_title">
            <h2>Model performance<hr></h2>
            <center><img src="/imgs/wp1880337.jpg" style="width: 700px;"></center>
            <p class="dataset_description">To test the abilities of the dataset in action, we trained 3 separate models. In the above picture, the performance can be seen.
                This figure shows that although vicuna has good performance on 2/5 sections, flan-t5 models perform more consistently across all of the tasks.
            </p>
        </div>
        

        <br><br><br><br>
        <div class="section_title">
            <h1>Citation<hr></h1>
            <p class="dataset_citation">If you use Vision-Flan in your research, please cite the following papers.</p>
            <center><div class="bibtex">
                <pre><code>
    @misc{visionFlan2023,
        title = {Vision-Flan:Scaling Visual Instruction Tuning},
        url = {https://vision-flan.github.io/},
        author = {Zhiyang Xu and Trevor Ashby and Chao Feng and Rulin Shao and Ying Shen and Di Jin and Qifan Wang and Lifu Huang},
        month = {Sep},
        year = {2023}
    }
                </code></pre>
                <pre><code>
    @inproceedings{DBLP:conf/acl/XuSH23,
        author       = {Zhiyang Xu and
                        Ying Shen and
                        Lifu Huang},
        editor       = {Anna Rogers and
                        Jordan L. Boyd{-}Graber and
                        Naoaki Okazaki},

        title        = {MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction
                        Tuning},

        booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                        Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                        July 9-14, 2023},

        pages        = {11445--11465},
        publisher    = {Association for Computational Linguistics},
        year         = {2023},
        url          = {https://doi.org/10.18653/v1/2023.acl-long.641},
        doi          = {10.18653/v1/2023.acl-long.641},
        timestamp    = {Thu, 10 Aug 2023 12:35:59 +0200},
        biburl       = {https://dblp.org/rec/conf/acl/XuSH23.bib},
        bibsource    = {dblp computer science bibliography, https://dblp.org}
    }
                </code></pre>
            </div></center>
        </div>
        <br><br><br><br>
        <div class="section_title" id="acknowledgement">
            <h1>Acknowledgement<hr></h1>
            <p class="dataset_acknowledgement"><span style="color:red;">Vision-Flan dataset is for research purpose only. 
                Please carefully check the licenses of the original datasets before using Vision-Flan.</span> 
                We provide the URLs to the original datasets and their Bibtex on this <a href="./bibtex.html">page</a>. 
                <b>We do not take credit for the previous works, and we want to thank the original authors for their contributions.</b>
                The images and tasks may be taken down at any time when requested by the original 
                dataset owners or owners of the referenced images. If you hope to take 
                down any tasks or the images, please contact Zhiyang Xu and Lifu Huang at <span class="email_text">zhiyangx@vt.edu</span> and <span class="email_text">lifuh@cs.vt.edu</span>.
            </p>
            <h3>Annotators<hr></h3>
            <p>We want to thank all of our annotators for the work that they contributed.</p>
        </div>
	</body>
</html>